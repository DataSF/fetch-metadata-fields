{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from optparse import OptionParser\n",
    "from ConfigUtils import *\n",
    "from SocrataStuff import *\n",
    "from Utils import *\n",
    "from UpdateSocrataFields import *\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BuildDatasets:\n",
    "    @staticmethod\n",
    "    def getQryCols(tables, dataset_name):\n",
    "        dataset = tables[dataset_name]\n",
    "        dataset_cols = dataset['field_list']\n",
    "        dataset_cols_qry = \",\".join(dataset_cols)\n",
    "        #if 'filter_qry' in dataset.keys():\n",
    "        #    dataset_cols_qry = dataset_cols_qry + dataset['filter_qry']\n",
    "        return dataset_cols_qry\n",
    "            \n",
    "    @staticmethod\n",
    "    def makeDf(json_obj):\n",
    "        df = json_normalize(json_obj)\n",
    "        df = df.fillna('')\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def getDatasets(tables, socrataQueriesObject):\n",
    "        dfs_dict = {}\n",
    "        datasets = tables.keys()\n",
    "        for dataset in datasets:\n",
    "            fbf = tables[dataset]['fbf']\n",
    "            qryCols = BuildDatasets.getQryCols(tables, dataset)\n",
    "            results_json = socrataQueriesObject.pageThroughResultsSelect(fbf, qryCols)\n",
    "            df = BuildDatasets.makeDf(results_json)\n",
    "            #rename the fields\n",
    "            if 'field_mapping' in tables[dataset].keys():\n",
    "                df = PandasUtils.mapFieldNames(df, tables[dataset]['field_mapping'])\n",
    "            dfs_dict[dataset] = df        \n",
    "        return dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fieldConfigFile = 'fieldConfigMasterDD.yaml'\n",
    "config_inputdir = '/Users/j9/Desktop/fetch-socrata-fields/configs/'\n",
    "\n",
    "cI =  ConfigItems(config_inputdir ,fieldConfigFile  )\n",
    "configItems = cI.getConfigs()\n",
    "sc = SocrataClient(config_inputdir, configItems)\n",
    "client = sc.connectToSocrata()\n",
    "clientItems = sc.connectToSocrataConfigItems()\n",
    "lg = pyLogger(configItems)\n",
    "logger = lg.setConfig()\n",
    "socrataLoadUtils = SocrataLoadUtils(configItems)\n",
    "scrud = SocrataCRUD(client, clientItems, configItems, logger)\n",
    "sqry = SocrataQueries(clientItems, configItems)\n",
    "\n",
    "\n",
    "tables = configItems['tables']\n",
    "dfs_dict = BuildDatasets.getDatasets(tables, sqry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asset_fields': 7727, 'asset_inventory': 4978, 'data_dictionary_attachments': 167, 'global_fields': 193, 'data_dictionary_do_not_process': 8, 'cordinators': 66, 'dataset_inventory': 862, 'department_lookup': 56}\n"
     ]
    }
   ],
   "source": [
    "df_sizes = {}\n",
    "df_names = dfs_dict.keys()\n",
    "for dataset in df_names:\n",
    "    df_sizes[dataset] = len(dfs_dict[dataset])\n",
    "print df_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "master_data_dictionary = configItems['master_data_dictionary']\n",
    "fields_to_include = master_data_dictionary['fields_to_include']\n",
    "base_url = configItems['base_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MasterDataDictionary:\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_base_datasets(dfs_dict):\n",
    "        '''filter out irrelevant records in datasets'''\n",
    "        #filter out the relevant data:\n",
    "        asset_fields = dfs_dict['asset_fields']\n",
    "        asset_inventory =  dfs_dict['asset_inventory']\n",
    "        data_dictionary_attachments = dfs_dict['data_dictionary_attachments']\n",
    "        dataset_inventory = dfs_dict['dataset_inventory']\n",
    "        coordinators = dfs_dict['cordinators']  \n",
    "        #asset_inventory-> we only want rows that are datsets\n",
    "        asset_inventory = asset_inventory[asset_inventory['type']== 'dataset']\n",
    "        #asset_inventory-> remove all the geo fields\n",
    "        asset_fields = asset_fields[asset_fields['data_type'] ==  'tabular']\n",
    "        print len(asset_fields)\n",
    "        #filter out records that don't have data_dictionaries + remove dupes\n",
    "        data_dictionary_attachments = data_dictionary_attachments[data_dictionary_attachments['data_dictionary_attached'] ==  True]\n",
    "        data_dictionary_attachments = data_dictionary_attachments.drop_duplicates('datasetid')\n",
    "        #grab the primary data coordinator\n",
    "        coordinators = coordinators[coordinators['primary'] == 'Yes']\n",
    "        return asset_fields, asset_inventory, data_dictionary_attachments, dataset_inventory, coordinators \n",
    "     \n",
    "    @staticmethod\n",
    "    def buildInventoryInfo(dataset_inventory,cordinators):\n",
    "        '''joins the dataset inventory to the coordinators dataset'''\n",
    "        dataset_inventory = pd.merge(dataset_inventory, cordinators, left_on='department_from_inventory', right_on='department', how='left')\n",
    "        return dataset_inventory\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_base(dfs_dict):\n",
    "        '''builds up all the non-transformed fields'''\n",
    "        fields_to_include= ['columnid', 'datasetid', 'dataset_name', 'inventoryid', 'field_name', 'socrata_field_type', 'field_type', 'api_key', 'data_steward', 'data_steward_name', 'department_from_inventory', 'department_from_catalog', 'data_coordinator', 'data_dictionary_attached', 'attachment_url', 'field_description']\n",
    "        asset_fields, asset_inventory, data_dictionary_attachments, dataset_inventory, coordinators = MasterDataDictionary.filter_base_datasets(dfs_dict)\n",
    "        dataset_inventory =  MasterDataDictionary.buildInventoryInfo(dataset_inventory,coordinators)\n",
    "        #join everything together to make the master dataset\n",
    "        master_df = pd.merge(asset_fields, asset_inventory, on='datasetid', how='left').merge(data_dictionary_attachments, on='datasetid', how='left').merge(dataset_inventory, on='datasetid', how='left')\n",
    "        print len(master_df)\n",
    "        master_df = master_df.fillna('')\n",
    "        master_df = master_df[fields_to_include ]\n",
    "        return master_df\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def add_url(master_df, base_url):\n",
    "        master_df['open_data_portal_url'] =  \"http://\"+ base_url+ '/resource/' + master_df['datasetid']\n",
    "        return master_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def calcDepartmentField(master_df):\n",
    "        df_master['department'] = \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def addCalculatedFields(master_df, base_url):\n",
    "        #fields_calculate= ['department', 'field_documented', 'global_field', 'do_not_process', 'field_definition', 'open_data_portal_url']\n",
    "        master_df = MasterDataDictionary.add_url(master_df, base_url)\n",
    "        return master_df\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def groupbyCountStar(df, group_by_list):\n",
    "    return df.groupby(group_by_list).size().reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6908\n",
      "6908\n",
      "6908\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# \tdatasetid\n",
    "mm_df = MasterDataDictionary.build_base(dfs_dict )\n",
    "print len(mm_df)\n",
    "mm_df = MasterDataDictionary.addCalculatedFields(mm_df, base_url)\n",
    "#mm_df\n",
    "\n",
    "#asset_data = dfs_dict['asset_fields']\n",
    "#asset_data = asset_data[asset_data['data_type'] ==  'tabular']\n",
    "#asset_data_geo = asset_data[asset_data['data_type'] ==  'geo']\n",
    "#asset_data_geo\n",
    "\n",
    "#column_id_cnts = groupbyCountStar(data_dictionary_attachments, ['datasetid'])\n",
    "#column_id_cnts = column_id_cnts[column_id_cnts['count'] != 1]\n",
    "#column_id_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
